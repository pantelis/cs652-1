## Control Plane
<!---Theodore Wagner CS652--->
Considering the control plane in the new datacenter architectures, at a high level, IP addressing and other layer 3 mechanisms won out, becoming the basis for forwarding between the different switches to the end server. Early on in the planning of their datacenter network architectures, Google and Facebook diverged. Both companies started by evaluating the use of existing protocols versus developing their own. In the case of Google, the decision was to develop their own. In the case of Facebook, they chose to use standard protocols that were already established. These decisions are broken down below.

### Google
#### Protocols
Google identifies a number of reasons for their decision to build their own control plane. One of the biggest motivators at the outset for developing their own control plane was to leverage the fact that the network was homogeneous. This presents some benefits which can be used to their advantage when customizing the control plane.

Enumerating some more specific reasons behind the decision to build their own control plane:

1. Existing routing protocols did not have good support for multipath, equal cost routing.
2. There were no high quality open source routing stacks, and it proved to be a lot of work to modify the hardware to support running the control-protocol packets through the switch setup.
3. Broadcast-based routing protocols across a large fabric of scale might produce a significant amount of overhead.
4. Network management would be simpler using a customized solution, as opposed to maintaining hundreds of independent switch stacks and configurations.

In their control plane organization, Google utilized an out-of-band Control Plane Network (CPN), across which a centralized router-controller would collect dynamic link state information and redistribute to all the switches. The switches would then recalculate their forwarding tables based on the incoming link state information, as well as knowing the (static) topology. For ease of control purposes, Google considered its datacenter network as one single large fabric with thousands of ports as opposed to individual switches that must learn about the fabric. The control mechanisms for Google's datacenter network consist primarily of their custom protocol called Firepath, but it also leverages the external Border Gateway Protocol for use communicating with external networks. 

##### Firepath
Firepath is the core protocol that provides the key functionality for Google's datacenter. In implementation, there is a Firepath client running on each fabric switch, and a set of redundant Firepath masters running on a selected subset of the spine switches. Specifically, it allows for neighbor discovery between neighbor switches, a keepalive functionality (i.e., a heartbeat function), exchange of link state information from the master controller, and standard interior routing between switches, all of which communication happens across the CPN. Firepath also contains a master redundancy function between the set of masters only.

|Function|Description|
|--------|-----------|
|Neighbor Discovery (ND)|Exchange of local port ID, expected peer port ID, discovered peer port ID, and link error signal to identify any issues with online liveness (i.e., if a peer is unavailable) and peer correctness (i.e., to make sure that cabling is correct). This information is propagated to the Interface Manager module on each switch and used to determine if a port is up. The port may be considered up only if it is physically up (PHY UP) and ND is correct (ND UP).|
|keepalive|Used both between switches and between the master controller and switches. Between switches, can determine if a peer is unavailable, which will eventually be reported and the routing paths adjusted. Between the master controller and switches, it is used in updating the link state database. The master controller periodically sends out messages containing its master ID and the current link state database number. If a client switch has lost synchronization with the master controller, it asks for the full link state database.|
|Link state exchange|Each Firepath client sends information about its interface state to the master controller across the CPN. Then the master controller constructs the link state database with a sequential version number every time there are routing changes to be made, due to planned and unplanned network events. The master sends out the link state database via UDP/IP to each client, and the clients recompute shortest path forwarding using Equal-Cost Multipath (ECMP).|
|Interior routing|Layer 3 subnets are assigned to top of rack switches to aid aggregation in the limited forwarding tables in merchant silicon. Layer 2 subnets are assigned to reduce the broadcast domain. Using ECMP and the link state information received from the master controller, switches determine the shortest path to reach other destinations, and route accordingly.
|Firepath Master Redundancy Protocol (FMRP)|The centralized master is a critical component of the Firepath system, collecting and distributing interface state information and synchronizing the Firepath clients. FMRP handles election of the master and maintains convergence between the set of active and backup masters.|

##### External Border Gateway Protocol (eBGP)
eBGP is used to handle communication with external networks. In the architecture, there is an isolated layer of switches to peer with the external routers. These routers integrate BGP with Firepath to enable the BGP stack on these external routers to communicate inband and to support the exchange of route information between BGP and Firepath. Therefore, the network architecture allows for outbound communication to be managed in one route through Firepath.

#### Configuration and Management
Google's goals for configuration and management were in favor of simplicity and reproducability over flexibility. Accordingly, the approach for configuration generation became very holistic, treating it again as a single fabric with specifically assigned roles to each switch. Therefore, the configuration became little more than a menu of fabric sizes and options. The configuration is pushed out as a single monolith, with each switch extracting out its relevant portion based on the assigned role.

On managing the individual switches, Google's approach was to make the switches look like regular machines, including in image management and installation, monitoring, logging, and alerting. Rather than employing standard network management protocols, Google decided to utilize protocols that would integrate with their existing server management infrastructure. External management of their switches is done through a single Common Management Access Layer (CMAL) interface, which allows for an administrator to drain or disable specific ports, and to implement a new switch configuration, which is done in a two-phase verify-commmit protocol. Management clients receive switch status updates through an API. Furthermore, there is a command line interface used for an operator to read the status of the switch, a minimal SNMP agent to support legacy SNMP monitoring, and a special monitoring agent that integrates to the existing monitoring infrastructure.

On managing the fabric, the approach of using the existing infrastructure remained consistent. The teams built tools that were aware of the fabric as a whole, simplifying the management process. The focus on development was on key components that were specific to their large scale network deployment, including link/switch qualification, fabric expansion/upgrade, and network troubleshooting at scale. On fabric upgrades, Google explored two options: dividing the topology into four sets or eight sets, in an attempt to balance fabric capacity degradation with time necessary for upgrading. On troubleshooting, the teams designed extensions to traceroute and ICMP to be fabric-aware, and can use these tools to run probes across servers randomly to detect anomaly flows.

### Facebook
#### Protocols
Where Google's decision was to build their own protocol, Facebook determined that they would use existing protocols to support their network operations. Facebook's approach was to look at the overall network first, then translate the necessary actions to the individual devices in the topology. Facebook built their network topology using BGP as the only routing protocol, using only minimum necessary features. This decision allowed for them to distribute the control plane, offer better routing propagation management, and ensure compatibility. They also employed a centralized BGP controller which can override any routing paths, giving their hybrid approach the descriptor "distributed control, centralized override."

All of the addressing and routing is done using IP addressing, built to be dual stack to natively support both IPv4 and IPv6. The fabric also uses ECMP routing with flow-based hashing, to ensure that load is distributed across the different concurrent flows. Should any flow become monopolized, there are mechanisms in place to route around these bottlenecks.

#### Configuration and Management
As with any Clos architecture, the *modus operandi* is uniformity and scalability. Facebook caters to the scalability piece by recognizing that they don't need to build out the full network capacity immediately. The design for the network deployments has the space and physical resources to build out a fully non-oversubscribed network, but at initial deployment, the number of spines is decreased from 48 to 12, resulting in an oversubscription ratio of 4:1, but still allowing for the same forwarding capacity as the previous cluster-based topology. Then, to scale up, either rapidly or sequentially, they simply need to add more spines. For backwards compability, Facebook preserved the logical concept of a "cluster" in their architecture, but implemented it as a collection of pods that may be dispersed across the data center. This allowed for a smoother transition to the new network architecture, while still leveraging the new benefits. One of these benefits is that they were able to roll out the fabric faster than the equivalent amount of clusters would have been, with no distruption to their production network.

On the idea of uniformity, Facebook leverages this into better programmability, for the purpose of more automation in the network, including for configurations. The tools they have developed can be applied to networks of any size, as the tools are independent of the forwarding plane; that is, a separate control plane. Configuration happens at the fabric level, not the device level, and features only the minimum settings needed to define the network, its components, and routing logic. The specific component parameters are derived from these high-level settings, converted to a format useful for the switches, and pushed out automatically. 

Considering troubleshooting, the focus falls on baselines and outliers, actively identifying problems and priority alerts and automatically remediating them. For each type of component, automatic rules and push-buttons actions are defined to gracefully take it out of service or redeploy it into production. The focus is on maintaining the overall behavior of the fabric, not focuing on individual boxes or links.
